{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9798ec5b-b717-44c0-a83d-142db0a8ab8f","showTitle":false,"title":""}},"source":["# Example notebook for data and model versioning using MLFlow and DeltaLake\n","\n","This note book is created based on [Databricks example notebook](https://docs.databricks.com/_static/notebooks/mlflow/mlflow-delta-training.html)\n","\n","The data used is loan status from lending club, 2007--2017, which can be found [here](https://www.kaggle.com/husainsb/lendingclub-issued-loans?select=lc_loan.csv)\n","\n","**This notebook is used to demo**\n","- Data versioning using Delta Lake and MLflow\n","- MLflow version a model and its dependent model (i.e. Logistic Regression and Lime in this case)\n","- Copy both artifacts from MLflow to a specified location\n","\n","\n","**Cluster Configuration**\n","- Databricks Runtime Version `6.4 ML (includes Apache Spark 2.4.5, Scala 2.11)`\n","- Additional Libraries needed, both are using `default` repository\n","  - From Maven Repo, coordinate `Azure:mmlspark:0.17`\n","  - From Pip install, `mlflow==1.14.1`\n","  \n","**Storage Requirement**\n","- If no credential passthrough is used, please mount the containers in the storage account and change the container name accordingly\n","- If credential passthrough is used, please make sure you have configured\n","  - When creating clusters, tick the box of \"using credential passthrough\"\n","  - Make sure the correct permission has been set to the storage account"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d40d2d27-38ff-4f4a-bcc1-8c36b46380d9","showTitle":false,"title":""}},"outputs":[],"source":["from distutils.version import LooseVersion\n","import pyspark\n","from mmlspark import TabularLIME, TabularLIMEModel\n","from pyspark.ml import Pipeline, PipelineModel\n","from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, StandardScaler, Imputer\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n","from pyspark.sql.types import FloatType\n","from pyspark.sql import DataFrame\n","from pyspark.sql import functions as F\n","from typing import Tuple, List\n","from pathlib import Path\n","\n","import mlflow\n","import mlflow.spark"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"25ebac86-37b8-4a42-845d-b5eca8b47356","showTitle":false,"title":""}},"outputs":[],"source":["print(f\"MLFlow version = {mlflow.__version__}\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ed627e58-51b2-43a8-a24d-0e869e06b606","showTitle":false,"title":""}},"source":["# Load data from Delta Lake\n","\n","To load data, in this example we use credential passthrough\n","- the mount Azure Delta Lake storage to Databricks (skip if we are using credential pass through)\n","  - If you are using credential passthrough please set  `IS_CRED_PASS` to `True`\n","- specify path and data version needed"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"30f21a90-fdb4-4c00-b277-6ff8c50917c7","showTitle":false,"title":""}},"source":["## Set crendential passthrough (skip if you already mounted the storage)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"91cfbd6b-2e69-40fc-a3ee-8d3f5b3d6b99","showTitle":false,"title":""}},"outputs":[],"source":["IS_CRED_PASS = True\n","data_path = \"/mnt\"\n","cred_passthrough_configs = {\n","\"fs.azure.account.auth.type\": \"CustomAccessToken\",\n","\"fs.azure.account.custom.token.provider.class\": spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\")\n","}\n","container_name = \"datalake\"\n","if IS_CRED_PASS:\n","  data_path = f\"abfss://{container_name}@dlsloandev.dfs.core.windows.net/lc_loan\"  "]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"73a1835b-1c6a-430d-b488-85bbb1a2c960","showTitle":false,"title":""}},"source":["## Specify path and version\n","\n","- Azure Delta Lake storage is mounted under path `/mnt/delta-ds-test/lc_loan` ï¼ˆor a direct access through credential pass through using container `delta-ds-test`). Note this is the default value and you can change it to the actual name of your deltalake container\n","- Data is ingested by year, from 2007 to 2012, using \"issue_d\" column as watermark for versioning."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4a066390-471c-4755-872a-4fbd8b156659","showTitle":false,"title":""}},"outputs":[],"source":["DEFAULT_DATA_CONTAINER = \"delta-ds-test\"\n","DEFAULT_ARTIFACT_CONTAINER = \"model-artifacts\"\n","dbutils.widgets.text(name=\"deltaVersion\", defaultValue=\"6\", label=\"Table version, default=6\")\n","dbutils.widgets.text(name=\"tbl_name\", defaultValue=\"delta-ds-test/lc_loan\", label=\"tbl_name,default=delta-ds-test/lc_loan\")\n","data_version = None if dbutils.widgets.get(\"deltaVersion\") == \"\" else int(dbutils.widgets.get(\"deltaVersion\"))\n","DELTA_TABLE_DEFAULT_PATH =  f\"/mnt/{DEFAULT_DATA_CONTAINER}/lc_loan\"\n","input_str = dbutils.widgets.get(\"tbl_name\")\n","data_path = \"\"\n","if input_str == \"\":\n","  data_path = f\"abfss://{container_name}@dlsloandev.dfs.core.windows.net/lc_loan\" if IS_CRED_PASS else DELTA_TABLE_DEFAULT_PATH\n","else:\n","  container_name, tbl_name = input_str.strip().split(\"/\")\n","  data_path = f\"abfss://{container_name}@dlsloandev.dfs.core.windows.net/{tbl_name}\" if IS_CRED_PASS else f\"/mnt/{input_str}\"\n","displayHTML(f\"Current data path {data_path}, version {data_version}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2b3cd173-687b-4c03-aab2-7dfa33588670","showTitle":false,"title":""}},"outputs":[],"source":["displayHTML(data_path)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6fac1af7-4641-4776-b2e0-7153d570a53a","showTitle":false,"title":""}},"source":["## Load from Delta Lake"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9f90640a-f84c-4799-817e-deeb63c7ba62","showTitle":false,"title":""}},"outputs":[],"source":["dataset = spark.read.format(\"delta\").option(\"versionAsOf\", data_version).load(data_path)\n","display(dataset.select('issue_d').withColumn('year', F.year('issue_d')).select('year').distinct())"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5ac82fe4-ecf3-4085-bba0-54d0ff085d36","showTitle":false,"title":""}},"outputs":[],"source":["spark.catalog.clearCache()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"aee416b1-3466-46c1-9284-e8859a818836","showTitle":false,"title":""}},"source":["# Data Transformation"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4ffe345d-15fd-4e16-973b-56288073fc83","showTitle":false,"title":""}},"source":["## Create bad loan label\n","\n","Create bad loan label, this will include charged off, defaulted, and late repayments on loans."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4b3a82c7-c682-4491-addb-d5a76cc3687d","showTitle":false,"title":""}},"outputs":[],"source":["dataset = dataset.filter(dataset.loan_status.isin([\"Default\", \"Charged Off\", \"Fully Paid\"]))\\\n","                       .withColumn(\"bad_loan\", (~(dataset.loan_status == \"Fully Paid\")).cast(\"string\"))"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"57f3f4de-c658-4db8-b0c3-96c5a63f2a95","showTitle":false,"title":""}},"source":["## Feature Engineering\n","\n","- use only year information\n","- compute credit_length_in_years\n","- add a \"net\" column as a new feature"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6b988bc3-04e6-4087-be00-3c886305bc3d","showTitle":false,"title":""}},"outputs":[],"source":["dataset = (\n","          dataset.withColumn('issue_year',  F.year(F.col('issue_d')).cast('double')) \n","                 .withColumn('earliest_year', F.year(F.col('earliest_cr_line')).cast('double'))\n","                  .withColumn('credit_length_in_years', F.col('issue_year')-F.col('earliest_year'))\n","           .withColumn('net', F.round(F.col('total_pymnt') -F.col('loan_amnt'), 2))\n","\n","          )"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e5e564f5-347b-4730-afed-ca88ac1d7272","showTitle":false,"title":""}},"outputs":[],"source":["display(dataset)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"15f0738a-608d-4ede-850f-601e7241969b","showTitle":false,"title":""}},"source":["# Helper functions for training"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6a93b7dd-b569-4f8b-b2ad-360b9d48ee99","showTitle":false,"title":""}},"source":["## Data Transformation\n","\n","- impute columns\n","- create feature vector\n","- convert target column to label"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"74ffb150-ab01-4ecf-b584-aed7576c9ae3","showTitle":false,"title":""}},"outputs":[],"source":["def data_transform(features:list, \n","                    target:str, \n","                   train_df: DataFrame)->PipelineModel:\n","  \"\"\"\n","  - transform feature columns into a single vector type column named `features`\n","  - convert target column to label using \"string indexer\"\n","  - fit the transformation pipeline using training data\n","  \n","  :param features. list of feature column names\n","  :type features: list\n","  :param target: name of the target column\n","  :type str\n","  :param train_df. Train data frame for fit the transformation pipeline\n","  :type: Dataframe\n","  \"\"\"\n","  model_matrix_stages = [\n","    Imputer(inputCols = features, outputCols = features),\n","    VectorAssembler(inputCols=features, outputCol='features'),\n","    StringIndexer(inputCol=target, outputCol=\"label\")\n","  ]\n","  transform_pipeline = Pipeline(stages=model_matrix_stages)\n","  transform_pipeline_model = transform_pipeline.fit(train_df)\n","  return transform_pipeline_model\n","  \n","  "]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"aaa1d839-5f22-422b-9c68-ee874f4a34cd","showTitle":false,"title":""}},"source":["## Train Function"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"330fa0ec-d63f-4beb-843c-85dc75312e6b","showTitle":false,"title":""}},"outputs":[],"source":["def train(train_df:DataFrame, \n","          test_df: DataFrame,\n","          lr_params:dict, \n","          lime_params:dict,\n","          lime_output_col:str=\"weights\",\n","          lime_prediction_col:str=\"prediction\")->Tuple[LogisticRegressionModel, TabularLIMEModel]:\n","  \"\"\"\n","  Helper function that fits a CrossValidator model to predict a binary label\n","  `target` on the passed-in training DataFrame using the columns in `features`\n","  :param: train: Spark DataFrame containing training data\n","  :param: features: List of strings containing column names to use as features from `train`\n","  :param: target: String name of binary target column of `train` to predict\n","  :param: lime_output_col:str, output column of LIME model\n","  :param: lime_prediction_col, prediction column to be input to LIME\n","  \"\"\"\n","  {mlflow.log_param(\"lr_\"+param, val) for param,val  in lr_params.items()}\n","  {mlflow.log_param(\"lime_\"+param, val) for param,val  in lime_params.items()}\n","  #   \n","  lime_input_col=\"features\"\n","  #   \n","  lr = LogisticRegression(**lr_params, featuresCol = \"features\")\n","  #   \n","  training_pipeline =  Pipeline(stages=[lr])\n","  paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0, 0.001, 1, 10]).build()\n","  crossval = CrossValidator(estimator=training_pipeline,\n","                            estimatorParamMaps=paramGrid,\n","                            evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"),\n","                            numFolds=5)\n"," \n","  cvModel = crossval.fit(train_df)\n","  # evaluate on the test data\n","  evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n","  validation_res = evaluator.evaluate(cvModel.transform(test_df))\n","  # log using mlflow\n","  mlflow.log_metric('test_' + evaluator.getMetricName(), validation_res)\n","\n","  # Train LIME model\n","  lr_model = cvModel.bestModel\n","  # get the final model parameter for regParam\n","  mlflow.log_param(\"lr_regParam\", lr_model.stages[-1]._java_obj.getRegParam())\n","  lime = (TabularLIME().\n","                setModel(lr_model.stages[-1]).\n","                setPredictionCol(lime_prediction_col).\n","                setOutputCol(lime_output_col).\n","                setInputCol(lime_input_col).\n","                setParams(**lime_params)\n","               )\n","\n","  lime_model = lime.fit(train_df)\n","  # log models\n","  mlflow.spark.log_model(lr_model, \"lr_model\")\n","  mlflow.spark.log_model(lime_model, \"lime_model\")\n","  return lr_model, lime_model"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2c9f072e-009a-4b7f-9b20-62c7e04dab0e","showTitle":false,"title":""}},"source":["## LIME Helper Function\n","\n","Helper function for split lime output weight vector into corresponding feature columns"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"571746eb-69ab-47c2-9243-3c4dc03e2f2c","showTitle":false,"title":""}},"outputs":[],"source":["#Helper function for LIME\n","def splitVector(df_split: pyspark.sql.DataFrame, new_features: list) -> pyspark.sql.DataFrame: \n","  \"\"\"flatten LIME output \"splitcol\" wherein the importance of each feature \n","  used in model is represented as a sigle column in the returend dataframe, \n","  while remaining rest of the output columns, e.g. prediction\n","\n","  :param df_split: Dataframe to be split column wise\n","  :type df_split: pyspark.sql.DataFrame\n","  :param new_features: column name list in the split columns\n","  :type new_features: list\n","  :return: Dataframe converted\n","  :rtype: pyspark.sql.DataFrame \n","  \"\"\"\n","  \n","  schema = df_split.schema\n","  cols = df_split.columns\n","\n","  for col in new_features: # new_features should be the same length as vector column length\n","    schema = schema.add(col,FloatType(),True)\n","\n","  return spark.createDataFrame(df_split.rdd.map(lambda row: [row[i] for i in cols]+row.splitcol.tolist()), schema)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"454c9b74-88b9-4c27-92ff-f680b836a488","showTitle":false,"title":""}},"source":["# Training\n","\n","- Split train/test based on the year, we always reserve latest year for validation.\n","- Start training"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"47d84f92-854f-4005-a199-59367f483509","showTitle":false,"title":""}},"outputs":[],"source":["tot_year = sorted([x.issue_year for x in dataset.select('issue_year').distinct().collect()])\n","split_year = tot_year[-2]"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f8903c0e-dea5-47c8-87e5-106badaa25f7","showTitle":false,"title":""}},"outputs":[],"source":["# split train and validation based on the year\n","feature_cols = [\"loan_amnt\",  \"annual_inc\", \"dti\", \"delinq_2yrs\",\"total_acc\", \"credit_length_in_years\", 'net']\n","target_col = 'bad_loan'\n","train_df = dataset.select(feature_cols + [target_col]).where(F.col('issue_year')<= split_year)\n","test_df = dataset.select(feature_cols + [target_col]).where(F.col('issue_year')>split_year)\n","transform_model = data_transform(train_df=train_df, features=feature_cols, target=target_col)\n","train_t_df = transform_model.transform(train_df)\n","test_t_df = transform_model.transform(test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4dd182c9-13d0-41ac-8a29-9825da6f1419","showTitle":false,"title":""}},"outputs":[],"source":["# cache dataframe to avoid lazy eval running multiple times\n","train_t_df.cache().count()\n","test_t_df.cache().count()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"acce8b9d-2d28-4c83-81ac-ad1e75cb2d2a","showTitle":false,"title":""}},"outputs":[],"source":["display(test_t_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3ec54555-5fa6-44d0-8200-a70911afff3c","showTitle":false,"title":""}},"outputs":[],"source":["from mlflow.tracking import MlflowClient\n","user_name = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n","experiment_name = f\"/Users/{user_name}/loan_classification\"\n","mlflow.set_experiment(experiment_name)\n","with mlflow.start_run():\n","  lr_params = {\"labelCol\": \"label\", \"maxIter\":50}\n","  lime_params = {\"nSamples\": 1000, \"samplingFraction\": 0.3, \"regularization\":0.0}\n","  tags = {\n","          \"data_path\" : data_path,\n","          \"data_version\": data_version,\n","          \"train_test_split\": split_year\n","      }\n","  mlflow.set_tags(tags)\n","  # log note\n","  mlflow.set_tag(\"mlflow.note.content\", \n","                 \"one sub-run for loan prediction with lr and lime models with expr for param tuning\")\n","  mlflow.log_param(\"train_test_split\", split_year)\n","  mlflow.log_param(\"data_version\", data_version)\n","  lr_model, lime_model = train(train_t_df, test_t_df,lr_params, lime_params)\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"86e8a440-2556-456d-8ea5-0ba632b201d9","showTitle":false,"title":""}},"source":["# Check prediction output for LR and LIME"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"eea86d7b-a2ae-41de-aed3-142f973bc84b","showTitle":false,"title":""}},"source":["## Check output from LR"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"83a66c9b-f6d6-4a70-96a7-402b83f8f507","showTitle":false,"title":""}},"outputs":[],"source":["pred_df = lr_model.transform(test_t_df)\n","pred_df.cache().count()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a62fe4f8-88ec-4f20-a78d-c50f8f7fbae8","showTitle":false,"title":""}},"outputs":[],"source":["display(pred_df)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8e0991c5-11ba-47af-9cf4-b280c95a4d51","showTitle":false,"title":""}},"source":["## Check output from LIME"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d45642f7-1406-4a48-b6c7-652641307bfa","showTitle":false,"title":""}},"outputs":[],"source":["lime_df = lime_model.transform(pred_df.select('features'))\n","\n","dfLimeSel = lime_df.select('weights').withColumnRenamed('weights', 'splitcol')\n","dfSplit = splitVector(dfLimeSel, feature_cols )\n","\n","dfResult = dfSplit.drop(\"splitcol\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b02828c8-39cf-4243-8458-51ede505791f","showTitle":false,"title":""}},"outputs":[],"source":["display(dfResult)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7388143e-f1c5-44de-90bf-77d8640ac9cd","showTitle":false,"title":""}},"source":["# Retrieve and Copy Artifact\n","- Retrieve based specific runs and experiments\n","- Copy both LR and LIME model to a specified location"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8e19c995-512e-4b8b-a415-84c338222515","showTitle":false,"title":""}},"source":["## Helper function for cp artifacts"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1ab3f6a2-3929-4708-b52c-e18659140dcd","showTitle":false,"title":""}},"outputs":[],"source":["def copy_model(\n","    exp_id: str,\n","    run_id: str,\n","    model_name: str,\n","    filepath: str\n","):\n","    \"\"\"\n","    copy a model from mlflow artificts to a filelocaion\n","    :param exp_id: expr id to be retrieved\n","    :type exp_id: str\n","    :param run_id: run id to be retrieved\n","    :type run_id: str\n","    :param model_name: model name to be retrieved\n","    :type model_name: str\n","    :param filepath: filepath the model to be stored\n","    :type filepath: str\n","    \"\"\"\n","    \n","    model_path = f\"dbfs:/databricks/mlflow-tracking/{exp_id}/{run_id}/artifacts/{model_name}\"\n","    model = mlflow.spark.load_model(model_path)\n","    model.write().overwrite().save(filepath)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"371a8197-4c78-4dd9-a924-d3795d0d9fea","showTitle":false,"title":""}},"source":["## Query all runs"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"11c6b2ac-6f16-4fac-bd3d-deca7fa0d278","showTitle":false,"title":""}},"outputs":[],"source":["experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n","runs = mlflow.search_runs(experiment_ids=experiment_id, order_by=['metrics.avg_accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"248b9632-46d1-4483-832d-b92b898c6831","showTitle":false,"title":""}},"outputs":[],"source":["runs"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"96e2a7d6-d620-4662-abc0-ce011a77066c","showTitle":false,"title":""}},"source":["## Only get \"best\" runs for each CV run"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"42ec325d-3a31-44a1-85ca-9a7da5c34614","showTitle":false,"title":""}},"outputs":[],"source":["runs[runs['tags.mlflow.parentRunId'].isnull()]"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"05eb664e-1d64-40d6-831e-36662afe45cf","showTitle":false,"title":""}},"source":["## Copy LR model and its dependent LIME model"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8dfacc10-8fc7-4572-80a4-9f56d37aa21c","showTitle":false,"title":""}},"outputs":[],"source":["run_id = \"88e43402bf904f4a9de3266ef47236eb\"\n","model_names=['lime_model', 'lr_model']\n","\n","artifact_path = ( f\"abfss://{DEFAULT_ARTIFACT_CONTAINER}@dlsloandev.dfs.core.windows.net/{experiment_id}/{run_id}\" if \n","                 IS_CRED_PASS else \n","                 f'/mnt/{DEFAULT_ARTIFACT_CONTAINER}/{experiment_id}/{run_id}')\n","for model in model_names:\n","  try: \n","    dbutils.fs.ls(f\"{artifact_path}/{model}\")\n","    displayHTML(\"Already copied!\")\n","  except:\n","    displayHTML(f\"Copying {model} to \"f\"{artifact_path}/{model}\")\n","    copy_model(experiment_id,run_id,model_name=model, filepath=f\"{artifact_path}/{model}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"cea111d8-0d08-4db1-9ae8-2dabd6349e84","showTitle":false,"title":""}},"outputs":[],"source":["display(dbutils.fs.ls(artifact_path))"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9b1ad38e-be9a-49b9-84b0-77d4e79ef906","showTitle":false,"title":""}},"source":["## Re-load a model and test\n","\n","- We use LIME model as an example, re-load it from the artifact path and test it"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"279a5933-9ff2-4334-99b2-5ed32ee7f2ca","showTitle":false,"title":""}},"outputs":[],"source":["test_model = 'lime_model'\n","model_path = f'{artifact_path}/{test_model}'\n","lime_reloaded = PipelineModel.load(model_path)\n","lime_df2 = lime_reloaded.transform(pred_df.select('features'))\n","dfLimeSel = lime_df.select('weights').withColumnRenamed('weights', 'splitcol')\n","dfSplit = splitVector(dfLimeSel, feature_cols )\n","dfResult = dfSplit.drop(\"splitcol\")\n","dfResult.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"436f35c2-7bf8-455d-98b0-271ac2f8ff1f","showTitle":false,"title":""}},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"loan_classification","notebookOrigID":1293095669402272,"widgets":{"deltaVersion":{"currentValue":"5","nuid":"02fdaace-53f1-41bd-9862-7a8424d654c6","widgetInfo":{"defaultValue":"6","label":"Table version, default=6","name":"deltaVersion","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}},"tbl_name":{"currentValue":"datalake/lc_loan","nuid":"8066955a-809f-4046-85c6-3f98d4efa1d5","widgetInfo":{"defaultValue":"delta-ds-test/lc_loan","label":"tbl_name,default=delta-ds-test/lc_loan","name":"tbl_name","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}}}},"language_info":{}},"nbformat":4,"nbformat_minor":0}