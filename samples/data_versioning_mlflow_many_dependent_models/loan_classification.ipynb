{"cells":[{"cell_type":"markdown","source":["# Example notebook for data and model versioning using MLFlow and DeltaLake\n\nThis note book is created based on [Databricks example notebook](https://docs.databricks.com/_static/notebooks/mlflow/mlflow-delta-training.html)\n\nThe data used is loan status from lending club, 2007--2017, which can be found [here](https://www.kaggle.com/husainsb/lendingclub-issued-loans?select=lc_loan.csv)\n\n**This notebook is used to demo**\n- Data versioning using [DeltaLake](https://docs.microsoft.com/en-us/azure/databricks/delta/delta-intro) and [MLflow](https://mlflow.org/)\n- MLflow version a model and its dependent model (i.e. Logistic Regression and [LIME](https://homes.cs.washington.edu/~marcotcr/blog/lime/) in this case)\n- Copy both artifacts from MLflow to a specified location\n\n\n**Cluster Configuration**\n- Databricks Runtime Version `6.4 ML (includes Apache Spark 2.4.5, Scala 2.11)`\n- Additional Libraries needed, both are using `default` repository\n  - From Maven Repo, coordinate `Azure:mmlspark:0.17`\n  - From Pip install, `mlflow==1.14.1`\n  \n**Storage Requirement**\n- If no [credential passthrough](https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/) is used, please mount the containers in the storage account and change the container name accordingly\n- If credential passthrough is used, please make sure you have configured\n  - When creating clusters, tick the box of \"using credential passthrough\"\n  - Make sure the correct permission has been set to the storage account"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9798ec5b-b717-44c0-a83d-142db0a8ab8f"}}},{"cell_type":"code","source":["from distutils.version import LooseVersion\nimport pyspark\nfrom mmlspark import TabularLIME, TabularLIMEModel\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, StandardScaler, Imputer\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql import functions as F\nfrom typing import Tuple, List\nfrom pathlib import Path\n\nimport mlflow\nimport mlflow.spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d40d2d27-38ff-4f4a-bcc1-8c36b46380d9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(f\"MLFlow version = {mlflow.__version__}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25ebac86-37b8-4a42-845d-b5eca8b47356"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Load data from Delta Lake\n\nTo load data, in this example we use credential passthrough\n- the mount Azure Delta Lake storage to Databricks (skip if we are using credential pass through)\n  - If you are using credential passthrough please set  `IS_CRED_PASS` to `True`\n- specify path and data version needed"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed627e58-51b2-43a8-a24d-0e869e06b606"}}},{"cell_type":"markdown","source":["## Set crendential passthrough (skip if you already mounted the storage)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30f21a90-fdb4-4c00-b277-6ff8c50917c7"}}},{"cell_type":"code","source":["IS_CRED_PASS = True\ndata_path = \"/mnt\"\ncred_passthrough_configs = {\n\"fs.azure.account.auth.type\": \"CustomAccessToken\",\n\"fs.azure.account.custom.token.provider.class\": spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\")\n}\ncontainer_name = \"datalake\"\nif IS_CRED_PASS:\n  data_path = f\"abfss://{container_name}@dlsloandev.dfs.core.windows.net/lc_loan\"  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91cfbd6b-2e69-40fc-a3ee-8d3f5b3d6b99"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Specify path and version\n\n- Azure Delta Lake storage is mounted under path `/mnt/delta-ds-test/lc_loan` ï¼ˆor a direct access through credential pass through using container `delta-ds-test`). Note this is the default value and you can change it to the actual name of your deltalake container\n- Data is ingested by year, from 2007 to 2012, using \"issue_d\" column as watermark for versioning."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73a1835b-1c6a-430d-b488-85bbb1a2c960"}}},{"cell_type":"code","source":["DEFAULT_DATA_CONTAINER = \"delta-ds-test\"\nDEFAULT_ARTIFACT_CONTAINER = \"model-artifacts\"\ndbutils.widgets.text(name=\"deltaVersion\", defaultValue=\"6\", label=\"Table version, default=6\")\ndbutils.widgets.text(name=\"tbl_name\", defaultValue=\"delta-ds-test/lc_loan\", label=\"tbl_name,default=delta-ds-test/lc_loan\")\ndata_version = None if dbutils.widgets.get(\"deltaVersion\") == \"\" else int(dbutils.widgets.get(\"deltaVersion\"))\nDELTA_TABLE_DEFAULT_PATH =  f\"/mnt/{DEFAULT_DATA_CONTAINER}/lc_loan\"\ninput_str = dbutils.widgets.get(\"tbl_name\")\ndata_path = \"\"\nif input_str == \"\":\n  data_path = f\"abfss://{container_name}@dlsloandev.dfs.core.windows.net/lc_loan\" if IS_CRED_PASS else DELTA_TABLE_DEFAULT_PATH\nelse:\n  container_name, tbl_name = input_str.strip().split(\"/\")\n  data_path = f\"abfss://{container_name}@dlsloandev.dfs.core.windows.net/{tbl_name}\" if IS_CRED_PASS else f\"/mnt/{input_str}\"\ndisplayHTML(f\"Current data path {data_path}, version {data_version}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a066390-471c-4755-872a-4fbd8b156659"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["displayHTML(data_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b3cd173-687b-4c03-aab2-7dfa33588670"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Load from Delta Lake"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fac1af7-4641-4776-b2e0-7153d570a53a"}}},{"cell_type":"code","source":["dataset = spark.read.format(\"delta\").option(\"versionAsOf\", data_version).load(data_path)\ndisplay(dataset.select('issue_d').withColumn('year', F.year('issue_d')).select('year').distinct())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f90640a-f84c-4799-817e-deeb63c7ba62"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.catalog.clearCache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ac82fe4-ecf3-4085-bba0-54d0ff085d36"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Data Transformation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aee416b1-3466-46c1-9284-e8859a818836"}}},{"cell_type":"markdown","source":["## Create bad loan label\n\nCreate bad loan label, this will include charged off, defaulted, and late repayments on loans."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ffe345d-15fd-4e16-973b-56288073fc83"}}},{"cell_type":"code","source":["dataset = dataset.filter(dataset.loan_status.isin([\"Default\", \"Charged Off\", \"Fully Paid\"]))\\\n                       .withColumn(\"bad_loan\", (~(dataset.loan_status == \"Fully Paid\")).cast(\"string\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b3a82c7-c682-4491-addb-d5a76cc3687d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Feature Engineering\n\n- use only year information\n- compute credit_length_in_years\n- add a \"net\" column as a new feature"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57f3f4de-c658-4db8-b0c3-96c5a63f2a95"}}},{"cell_type":"code","source":["dataset = (\n          dataset.withColumn('issue_year',  F.year(F.col('issue_d')).cast('double')) \n                 .withColumn('earliest_year', F.year(F.col('earliest_cr_line')).cast('double'))\n                  .withColumn('credit_length_in_years', F.col('issue_year')-F.col('earliest_year'))\n           .withColumn('net', F.round(F.col('total_pymnt') -F.col('loan_amnt'), 2))\n\n          )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b988bc3-04e6-4087-be00-3c886305bc3d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(dataset)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5e564f5-347b-4730-afed-ca88ac1d7272"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Helper functions for training"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15f0738a-608d-4ede-850f-601e7241969b"}}},{"cell_type":"markdown","source":["## Data Transformation\n\n- impute columns\n- create feature vector\n- convert target column to label"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a93b7dd-b569-4f8b-b2ad-360b9d48ee99"}}},{"cell_type":"code","source":["def data_transform(features:list, \n                    target:str, \n                   train_df: DataFrame)->PipelineModel:\n  \"\"\"\n  - transform feature columns into a single vector type column named `features`\n  - convert target column to label using \"string indexer\"\n  - fit the transformation pipeline using training data\n  \n  :param features. list of feature column names\n  :type features: list\n  :param target: name of the target column\n  :type str\n  :param train_df. Train data frame for fit the transformation pipeline\n  :type: Dataframe\n  \"\"\"\n  model_matrix_stages = [\n    Imputer(inputCols = features, outputCols = features),\n    VectorAssembler(inputCols=features, outputCol='features'),\n    StringIndexer(inputCol=target, outputCol=\"label\")\n  ]\n  transform_pipeline = Pipeline(stages=model_matrix_stages)\n  transform_pipeline_model = transform_pipeline.fit(train_df)\n  return transform_pipeline_model\n  \n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74ffb150-ab01-4ecf-b584-aed7576c9ae3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Train Function"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aaa1d839-5f22-422b-9c68-ee874f4a34cd"}}},{"cell_type":"code","source":["def train(train_df:DataFrame, \n          test_df: DataFrame,\n          lr_params:dict, \n          lime_params:dict,\n          lime_output_col:str=\"weights\",\n          lime_prediction_col:str=\"prediction\")->Tuple[LogisticRegressionModel, TabularLIMEModel]:\n  \"\"\"\n  Helper function that fits a CrossValidator model to predict a binary label\n  `target` on the passed-in training DataFrame using the columns in `features`\n  :param: train: Spark DataFrame containing training data\n  :param: features: List of strings containing column names to use as features from `train`\n  :param: target: String name of binary target column of `train` to predict\n  :param: lime_output_col:str, output column of LIME model\n  :param: lime_prediction_col, prediction column to be input to LIME\n  \"\"\"\n  {mlflow.log_param(\"lr_\"+param, val) for param,val  in lr_params.items()}\n  {mlflow.log_param(\"lime_\"+param, val) for param,val  in lime_params.items()}\n  #   \n  lime_input_col=\"features\"\n  #   \n  lr = LogisticRegression(**lr_params, featuresCol = \"features\")\n  #   \n  training_pipeline =  Pipeline(stages=[lr])\n  paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0, 0.001, 1, 10]).build()\n  crossval = CrossValidator(estimator=training_pipeline,\n                            estimatorParamMaps=paramGrid,\n                            evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"),\n                            numFolds=5)\n \n  cvModel = crossval.fit(train_df)\n  # evaluate on the test data\n  evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n  validation_res = evaluator.evaluate(cvModel.transform(test_df))\n  # log using mlflow\n  mlflow.log_metric('test_' + evaluator.getMetricName(), validation_res)\n\n  # Train LIME model\n  lr_model = cvModel.bestModel\n  # get the final model parameter for regParam\n  mlflow.log_param(\"lr_regParam\", lr_model.stages[-1]._java_obj.getRegParam())\n  lime = (TabularLIME().\n                setModel(lr_model.stages[-1]).\n                setPredictionCol(lime_prediction_col).\n                setOutputCol(lime_output_col).\n                setInputCol(lime_input_col).\n                setParams(**lime_params)\n               )\n\n  lime_model = lime.fit(train_df)\n  # log models\n  mlflow.spark.log_model(lr_model, \"lr_model\")\n  mlflow.spark.log_model(lime_model, \"lime_model\")\n  return lr_model, lime_model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"330fa0ec-d63f-4beb-843c-85dc75312e6b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## LIME Helper Function\n\nHelper function for split lime output weight vector into corresponding feature columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c9f072e-009a-4b7f-9b20-62c7e04dab0e"}}},{"cell_type":"code","source":["#Helper function for LIME\ndef splitVector(df_split: pyspark.sql.DataFrame, new_features: list) -> pyspark.sql.DataFrame: \n  \"\"\"flatten LIME output \"splitcol\" wherein the importance of each feature \n  used in model is represented as a sigle column in the returend dataframe, \n  while remaining rest of the output columns, e.g. prediction\n\n  :param df_split: Dataframe to be split column wise\n  :type df_split: pyspark.sql.DataFrame\n  :param new_features: column name list in the split columns\n  :type new_features: list\n  :return: Dataframe converted\n  :rtype: pyspark.sql.DataFrame \n  \"\"\"\n  \n  schema = df_split.schema\n  cols = df_split.columns\n\n  for col in new_features: # new_features should be the same length as vector column length\n    schema = schema.add(col,FloatType(),True)\n  return spark.createDataFrame(df_split.rdd.map(lambda row: [row[i] for i in cols]+row.splitcol.tolist()), schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"571746eb-69ab-47c2-9243-3c4dc03e2f2c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Training\n\n- Split train/test based on the year, we always reserve latest year for validation.\n- Start training"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"454c9b74-88b9-4c27-92ff-f680b836a488"}}},{"cell_type":"code","source":["tot_year = sorted([x.issue_year for x in dataset.select('issue_year').distinct().collect()])\nsplit_year = tot_year[-2]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47d84f92-854f-4005-a199-59367f483509"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# split train and validation based on the year\nfeature_cols = [\"loan_amnt\",  \"annual_inc\", \"dti\", \"delinq_2yrs\",\"total_acc\", \"credit_length_in_years\", 'net']\ntarget_col = 'bad_loan'\ntrain_df = dataset.select(feature_cols + [target_col]).where(F.col('issue_year')<= split_year)\ntest_df = dataset.select(feature_cols + [target_col]).where(F.col('issue_year')>split_year)\ntransform_model = data_transform(train_df=train_df, features=feature_cols, target=target_col)\ntrain_t_df = transform_model.transform(train_df)\ntest_t_df = transform_model.transform(test_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8903c0e-dea5-47c8-87e5-106badaa25f7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# cache dataframe to avoid lazy eval running multiple times\ntrain_t_df.cache().count()\ntest_t_df.cache().count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4dd182c9-13d0-41ac-8a29-9825da6f1419"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(test_t_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acce8b9d-2d28-4c83-81ac-ad1e75cb2d2a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from mlflow.tracking import MlflowClient\nuser_name = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\nexperiment_name = f\"/Users/{user_name}/loan_classification\"\nmlflow.set_experiment(experiment_name)\nwith mlflow.start_run():\n  lr_params = {\"labelCol\": \"label\", \"maxIter\":50}\n  lime_params = {\"nSamples\": 1000, \"samplingFraction\": 0.3, \"regularization\":0.0}\n  tags = {\n          \"data_path\" : data_path,\n          \"data_version\": data_version,\n          \"train_test_split\": split_year\n      }\n  mlflow.set_tags(tags)\n  # log note\n  mlflow.set_tag(\"mlflow.note.content\", \n                 \"one sub-run for loan prediction with lr and lime models with expr for param tuning\")\n  mlflow.log_param(\"train_test_split\", split_year)\n  mlflow.log_param(\"data_version\", data_version)\n  lr_model, lime_model = train(train_t_df, test_t_df,lr_params, lime_params)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ec54555-5fa6-44d0-8200-a70911afff3c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Check prediction output for LR and LIME"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86e8a440-2556-456d-8ea5-0ba632b201d9"}}},{"cell_type":"markdown","source":["## Check output from LR"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eea86d7b-a2ae-41de-aed3-142f973bc84b"}}},{"cell_type":"code","source":["pred_df = lr_model.transform(test_t_df)\npred_df.cache().count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83a66c9b-f6d6-4a70-96a7-402b83f8f507"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(pred_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a62fe4f8-88ec-4f20-a78d-c50f8f7fbae8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Check output from LIME"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e0991c5-11ba-47af-9cf4-b280c95a4d51"}}},{"cell_type":"code","source":["lime_df = lime_model.transform(pred_df.select('features'))\ndfLimeSel = lime_df.select('weights').withColumnRenamed('weights', 'splitcol')\ndfSplit = splitVector(dfLimeSel, feature_cols )\ndfResult = dfSplit.drop(\"splitcol\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d45642f7-1406-4a48-b6c7-652641307bfa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(dfResult)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b02828c8-39cf-4243-8458-51ede505791f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Retrieve and Copy Artifact\n- Retrieve based specific runs and experiments\n- Copy both LR and LIME model to a specified location"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7388143e-f1c5-44de-90bf-77d8640ac9cd"}}},{"cell_type":"markdown","source":["## Helper function for cp artifacts"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e19c995-512e-4b8b-a415-84c338222515"}}},{"cell_type":"code","source":["def copy_model(\n    exp_id: str,\n    run_id: str,\n    model_name: str,\n    filepath: str\n):\n    \"\"\"\n    copy a model from mlflow artificts to a filelocaion\n    :param exp_id: expr id to be retrieved\n    :type exp_id: str\n    :param run_id: run id to be retrieved\n    :type run_id: str\n    :param model_name: model name to be retrieved\n    :type model_name: str\n    :param filepath: filepath the model to be stored\n    :type filepath: str\n    \"\"\"\n    \n    model_path = f\"dbfs:/databricks/mlflow-tracking/{exp_id}/{run_id}/artifacts/{model_name}\"\n    model = mlflow.spark.load_model(model_path)\n    model.write().overwrite().save(filepath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ab3f6a2-3929-4708-b52c-e18659140dcd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Query all runs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"371a8197-4c78-4dd9-a924-d3795d0d9fea"}}},{"cell_type":"code","source":["experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\nruns = mlflow.search_runs(experiment_ids=experiment_id, order_by=['metrics.avg_accuracy'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11c6b2ac-6f16-4fac-bd3d-deca7fa0d278"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["runs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"248b9632-46d1-4483-832d-b92b898c6831"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Only get \"best\" runs for each CV run"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96e2a7d6-d620-4662-abc0-ce011a77066c"}}},{"cell_type":"code","source":["runs[runs['tags.mlflow.parentRunId'].isnull()]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42ec325d-3a31-44a1-85ca-9a7da5c34614"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Copy LR model and its dependent LIME model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05eb664e-1d64-40d6-831e-36662afe45cf"}}},{"cell_type":"code","source":["run_id = \"your-run-id-here-from-mlflow\" #you can find your runid from mlflow\nmodel_names=['lime_model', 'lr_model']\n\nartifact_path = ( f\"abfss://{DEFAULT_ARTIFACT_CONTAINER}@dlsloandev.dfs.core.windows.net/{experiment_id}/{run_id}\" if \n                 IS_CRED_PASS else \n                 f'/mnt/{DEFAULT_ARTIFACT_CONTAINER}/{experiment_id}/{run_id}')\nfor model in model_names:\n  try: \n    dbutils.fs.ls(f\"{artifact_path}/{model}\")\n    displayHTML(\"Already copied!\")\n  except:\n    displayHTML(f\"Copying {model} to \"f\"{artifact_path}/{model}\")\n    copy_model(experiment_id,run_id,model_name=model, filepath=f\"{artifact_path}/{model}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8dfacc10-8fc7-4572-80a4-9f56d37aa21c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(dbutils.fs.ls(artifact_path))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cea111d8-0d08-4db1-9ae8-2dabd6349e84"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Re-load a model and test\n\n- We use LIME model as an example, re-load it from the artifact path and test it"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b1ad38e-be9a-49b9-84b0-77d4e79ef906"}}},{"cell_type":"code","source":["test_model = 'lime_model'\nmodel_path = f'{artifact_path}/{test_model}'\nlime_reloaded = PipelineModel.load(model_path)\nlime_df2 = lime_reloaded.transform(pred_df.select('features'))\ndfLimeSel = lime_df.select('weights').withColumnRenamed('weights', 'splitcol')\ndfSplit = splitVector(dfLimeSel, feature_cols )\ndfResult = dfSplit.drop(\"splitcol\")\ndfResult.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"279a5933-9ff2-4334-99b2-5ed32ee7f2ca"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"436f35c2-7bf8-455d-98b0-271ac2f8ff1f"}},"outputs":[],"execution_count":0}],"metadata":{"language_info":{},"application/vnd.databricks.v1+notebook":{"notebookName":"loan_classification","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{"deltaVersion":{"nuid":"02fdaace-53f1-41bd-9862-7a8424d654c6","currentValue":"5","widgetInfo":{"widgetType":"text","name":"deltaVersion","defaultValue":"6","label":"Table version, default=6","options":{"widgetType":"text","validationRegex":null}}},"tbl_name":{"nuid":"8066955a-809f-4046-85c6-3f98d4efa1d5","currentValue":"datalake/lc_loan","widgetInfo":{"widgetType":"text","name":"tbl_name","defaultValue":"delta-ds-test/lc_loan","label":"tbl_name,default=delta-ds-test/lc_loan","options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":3903031653641619}},"nbformat":4,"nbformat_minor":0}
