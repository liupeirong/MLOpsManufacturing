{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook is a quick utility for splitting the example Test Data CSV into a folder structure that better represents the expected folder structure of incoming data.\r\n",
    "It was created to facilitate demonstration of the Data Drift tools of Azure ML"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load in local test data CSV into Pandas\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "df = pd.read_csv('location-of-test-data-in-repository')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Remove special characters from column/feature names\n",
    "df.columns = df.columns.str.replace('[%,&,(,),*]', '')\n",
    "df.columns = df.columns.str.replace('[ ,  ]', '_')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Add a column for a date which strips off the Hours/Minutes of timestamp_x\n",
    "# This is used for splitting the data into each subfolder\n",
    "df['date'] = pd.to_datetime(df['timestamp_x'], format='%m/%d/%Y %H:%M').apply(lambda x: x.strftime('%Y/%m/%d') if x is not pd.NaT else None)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "# Create a new file for each day's worth of data\n",
    "for day in dates:\n",
    "    if day is not None:\n",
    "        splitdate = day.split('/')\n",
    "        year = splitdate[0]\n",
    "        month = splitdate[1]\n",
    "        day = splitdate[2]\n",
    "        minidf.to_csv(f'./{year}/{month}/{day}/data.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upload to Azure\r\n",
    "\r\n",
    "Once the local copy of the folder structure is created, you can quickly push it to blob storage using the Azure CLI:\r\n",
    "\r\n",
    "```azurecli\r\n",
    "az storage azcopy blob upload -c <NAME OF CONTAINER> --account-name <NAME OF STORAGE ACCOUNT> -s <wherever you have saved the split csv files> --recursive\r\n",
    "```\r\n",
    "\r\n",
    "\r\n",
    "## Register datastore\r\n",
    "\r\n",
    "Now that the data is uploaded into Azure, we can register the datastore (or, if you uploaded to a datastore that was already registered - as in this example, simply retrieve that datastore)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core import Datastore, Dataset, Workspace\n",
    "ws = Workspace('SUBSCRIPTION ID', 'RESOURCE GROUP', 'WORKSPACE NAME')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds = Datastore.get(ws, 'DATASTORE NAME')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Dataset\r\n",
    "\r\n",
    "Using wildcards to handle all of the date-based subfolders, we can register our Dataset as a tabular dataset.\r\n",
    "The `partition_format` option allows us to add columns to the Dataset based on the folder path. In this example, we add a \"line\" and \"upload_date\" column based on the folder structure."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "partitioned_dataset = Dataset.Tabular.from_delimited_files(path=[(ds, '*/*/*/data.csv')], partition_format='{line}/{upload_date:yyyy/MM/dd}/data.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rows = partitioned_dataset.take(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rows.to_pandas_dataframe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Register the Dataset\r\n",
    "\r\n",
    "The local version of the Tabular Dataset has been created, now we need to mark which columns (is/are) our timestamp(s), before registering them in Azure ML.\r\n",
    "\r\n",
    "The `timestamp` column is a \"fine grain\" timestamp which we can filter on, while the `partition_timestamp` is a \"course grain\" timestamp used for partitioning the data into groups. Since `timestamp_x` has hours and minutes included, here we will use that as our `timestamp` parameter, while the `partition_timestamp` will be the `date` column. The `partition_timestamp` is optional."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "partitioned_dataset = partitioned_dataset.with_timestamp_columns(timestamp='timestamp_x', partition_timestamp='date')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "partitioned_dataset.register(ws, 'PartitionedData', create_new_version=True)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}