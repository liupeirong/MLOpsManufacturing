{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Check drift installed"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip show azureml-datadrift"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core import Workspace\r\n",
        "\r\n",
        "# Load the workspace from the saved config file\r\n",
        "ws = Workspace.from_config()\r\n",
        "print('Ready to work with', ws.name)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625175967809
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Baseline dataset\r\n",
        "\r\n",
        "Upload file to default datastore, then make a new dataset from there."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core import Datastore, Dataset\r\n",
        "\r\n",
        "\r\n",
        "# Upload the baseline data\r\n",
        "# You'll need to upload/have your own sample .csv\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "default_ds.upload_files(files=['./data/sample.csv'],\r\n",
        "                       target_path='data-baseline',\r\n",
        "                       overwrite=True,\r\n",
        "                       show_progress=True)\r\n",
        "\r\n",
        "# Create and register the baseline dataset\r\n",
        "print('Registering baseline dataset...')\r\n",
        "baseline_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'data-baseline/*.csv'))\r\n",
        "baseline_data_set = baseline_data_set.register(workspace=ws,\r\n",
        "                           name='data baseline',\r\n",
        "                           description='baseline data',\r\n",
        "                           tags = {'format':'CSV'},\r\n",
        "                           create_new_version=True)\r\n",
        "\r\n",
        "print('Baseline dataset registered!')"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625176330375
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create target set with drift"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import datetime as dt\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "print('Generating simulated data...')\r\n",
        "\r\n",
        "# Load the smaller of the two data files\r\n",
        "data = pd.read_csv('data/sample.csv')\r\n",
        "\r\n",
        "# We'll generate data for the past 6 weeks\r\n",
        "weeknos = reversed(range(6))\r\n",
        "\r\n",
        "file_paths = []\r\n",
        "for weekno in weeknos:\r\n",
        "\r\n",
        "    # Get the date X weeks ago\r\n",
        "    data_date = dt.date.today() - dt.timedelta(weeks=weekno)\r\n",
        "\r\n",
        "    # Modify data to ceate some drift\r\n",
        "    # For each week, add drift to it\r\n",
        "    # These are fake features. Update corresponding to the features of your sample data\r\n",
        "    data['Feature 1'] = data['Feature 1'] + 2\r\n",
        "    data['Feature 2'] = round(data['Feature 2'] * 1.2).astype(int)\r\n",
        "\r\n",
        "    # Save the file with the date encoded in the filename\r\n",
        "    # Create a new file with the name of the date in the filename, turn it into csv and add it to an array to all be uploaded\r\n",
        "    # Each week file has features modified based on lines 20-23 above\r\n",
        "    file_path = 'data/sample_{}.csv'.format(data_date.strftime(\"%Y-%m-%d\"))\r\n",
        "    data.to_csv(file_path)\r\n",
        "    file_paths.append(file_path)\r\n",
        "\r\n",
        "# You have an array of file paths with each file being a from 6 weeks ago\r\n",
        "# Upload the files\r\n",
        "path_on_datastore = 'data-target'\r\n",
        "default_ds.upload_files(files=file_paths,\r\n",
        "                       target_path=path_on_datastore,\r\n",
        "                       overwrite=True,\r\n",
        "                       show_progress=True)\r\n",
        "\r\n",
        "# Use the folder partition format to define a dataset with a 'date' timestamp column\r\n",
        "partition_format = path_on_datastore + '/sample_{date:yyyy-MM-dd}.csv'\r\n",
        "target_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, path_on_datastore + '/*.csv'),\r\n",
        "                                                       partition_format=partition_format)\r\n",
        "\r\n",
        "# Register the target dataset\r\n",
        "print('Registering target dataset...')\r\n",
        "target_data_set = target_data_set.with_timestamp_columns('date').register(workspace=ws,\r\n",
        "                                                                          name='data target',\r\n",
        "                                                                          description='target data',\r\n",
        "                                                                          tags = {'format':'CSV'},\r\n",
        "                                                                          create_new_version=True)\r\n",
        "\r\n",
        "print('Target dataset registered!')"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625579894506
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create + Run Dataset Monitor"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "cluster_name = \"drift2\"\r\n",
        "\r\n",
        "try:\r\n",
        "    # Check for existing compute target\r\n",
        "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except ComputeTargetException:\r\n",
        "    # If it doesn't already exist, create it\r\n",
        "    try:\r\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\r\n",
        "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
        "        training_cluster.wait_for_completion(show_output=True)\r\n",
        "    except Exception as ex:\r\n",
        "        print(ex)"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625580207477
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.datadrift import DataDriftDetector\r\n",
        "\r\n",
        "# set up feature list\r\n",
        "# same features we drifted before\r\n",
        "\r\n",
        "# set up data drift detector\r\n",
        "monitor = DataDriftDetector.create_from_datasets(ws, 'data-drift-2', baseline_data_set, target_data_set,\r\n",
        "                                                      compute_target=cluster_name,\r\n",
        "                                                      frequency='Week',\r\n",
        "                                                      feature_list=None,\r\n",
        "                                                      drift_threshold=.3,\r\n",
        "                                                      latency=24)\r\n",
        "monitor"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625597158016
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azureml.widgets import RunDetails\r\n",
        "\r\n",
        "#backfill from 6 weeks ago, to today\r\n",
        "backfill = monitor.backfill(dt.datetime.now() - dt.timedelta(weeks=6), dt.datetime.now())\r\n",
        "\r\n",
        "RunDetails(backfill).show()\r\n",
        "backfill.wait_for_completion()"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625580972585
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "drift_metrics = backfill.get_metrics()\r\n",
        "for metric in drift_metrics:\r\n",
        "    print(metric, drift_metrics[metric])"
      ],
      "outputs": [],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625581083114
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}